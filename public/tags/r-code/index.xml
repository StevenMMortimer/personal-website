<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>R Code on Home</title>
    <link>/tags/r-code/</link>
    <description>Recent content in R Code on Home</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 28 Jul 2017 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/tags/r-code/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>A Scalable Predictive API with PFA and AWS Lambda</title>
      <link>/2017/07/a-scalable-predictive-api-with-pfa-and-aws-lambda/</link>
      <pubDate>Fri, 28 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/07/a-scalable-predictive-api-with-pfa-and-aws-lambda/</guid>
      <description>A Scalable Infrastructure Model Building and Execution using PFA Actually Doing This Step 1 - Creating the Lambda Function Step 2 - Build Your Model Step 3 - Upload Your Model to S3 Step 4 - Test Your Lambda Makes Predictions Step 5 - Hook Lambda Up to API Gateway  Load Testing Load Testing Results Load Testing Config    Building machine learning models is fun and challenging, but there are a whole other set of challenges when figuring out how to deploy your models in a production environment.</description>
    </item>
    
    <item>
      <title>Google Sites for Documentation</title>
      <link>/2017/06/google-sites-for-documentation/</link>
      <pubDate>Fri, 02 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/06/google-sites-for-documentation/</guid>
      <description>The WYSIWYG Access Control Permissions Embedded Google Docs, Sheets, etc. Built In Google Search and Analytics Final Thoughts   The title of this post is probably off-putting to a large number of people. Why on earth would I use Google Sites for blogging, wiki, and technical documentation? Isn’t that old, outdated tech even though Google did release a newer version? To be honest, I agree with those objections. This blog isn’t written with Google Sites.</description>
    </item>
    
    <item>
      <title>googlesites</title>
      <link>/projects/googlesites/</link>
      <pubDate>Thu, 01 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>/projects/googlesites/</guid>
      <description>Overview How to Get Started Add a page from HTML Find your Content Upload an Attachment to your Page Delete your Page  Additional Features Questions Support   Overview The googlesites package is the R implementation of the Google Sites API. Using this package assumes you’ve used the Web UI to create your site. Once you’ve got a site (and maybe some templates), you can use this package to add more content, add attachments, find content, and delete content.</description>
    </item>
    
    <item>
      <title>rdfp</title>
      <link>/projects/rdfp/</link>
      <pubDate>Mon, 22 May 2017 00:00:00 +0000</pubDate>
      
      <guid>/projects/rdfp/</guid>
      <description>Overview Features How to Get Started Install rdfp Library Setup and Authenticate Check Current User Info Setup Custom Fields for Items Setup Custom Targeting Keys and Values Create an Order Get Line Items By A Filter Run a Report  Check out the Tests Credits License Questions Support   Overview The rdfp package is the R implementation of the Double Click for Publishers (DFP) API and similar in comparison to the existing client libraries supported by Google.</description>
    </item>
    
    <item>
      <title>roas</title>
      <link>/projects/roas/</link>
      <pubDate>Sun, 21 May 2017 00:00:00 +0000</pubDate>
      
      <guid>/projects/roas/</guid>
      <description>Overview Features Functions How to Get Started Install roas Library Setup and Authenticate Authenticate with oas_build_credentials() Listing OAS Objects Reading an OAS Object Running Reports Getting Inventory Forecasts  Questions Support   Overview The roas package is the R implementation of the Open Ad Stream (OAS) API. I created the package because very few tools and documentation exist for the OAS API.
View source code on GitHub at: https://github.</description>
    </item>
    
    <item>
      <title>PFA with the R Package aurelius</title>
      <link>/2017/02/pfa-with-the-r-package-aurelius/</link>
      <pubDate>Mon, 13 Feb 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/02/pfa-with-the-r-package-aurelius/</guid>
      <description>I first heard about the Portable Format for Analytics (PFA) through KDnuggets in 2016. There was a post that in vague way talked about the virtues and benefits of PFA, but not to completely supplant PMML, which I was already familiar with. PMML was pretty cool, but I couldn’t find good support and felt that the XML-based approach was clumsy and outdated so when I heard about PFA, I was pretty excited.</description>
    </item>
    
    <item>
      <title>Creating an RStudio Addin</title>
      <link>/2016/12/creating-an-rstudio-addin/</link>
      <pubDate>Sat, 10 Dec 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/12/creating-an-rstudio-addin/</guid>
      <description>I never thought too much about RStudio addins, but I saw a few cool examples ( jadd, colorpicker, and addinslist) and decided to take a closer look. After a little research, it seemed easier than I thought. It was easier because RStudio provided some excellent documentation on addins and it turns out that if you can write R code, then you can write an RStudio addin.
My Idea Around the time I was looking at addins, I started taking advantage of a little RStudio trick to navigate long scripts.</description>
    </item>
    
    <item>
      <title>Deduping Records</title>
      <link>/2016/12/deduping-records/</link>
      <pubDate>Thu, 01 Dec 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/12/deduping-records/</guid>
      <description>Common Data Deduping Logic
A while ago I was working on migration of 4 separate legacy Salesforce systems into a single org. Naturally, the lack of consistency and controls led to each of those systems containing slighly different versions of the same data. A requirement for the migration was to remove any duplicate records and preserve the best version of the truth for a single entity. This concept is known as master data management.</description>
    </item>
    
    <item>
      <title>The Unfinished duplicated Function</title>
      <link>/2016/11/the-unfinished-duplicated-function/</link>
      <pubDate>Sun, 20 Nov 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/11/the-unfinished-duplicated-function/</guid>
      <description>If you’re a regular R user, then you’ve probably used or have seen the function duplicated(). If you’ve only grown up in the world of dplyr and other tidyverse packages with its cool function distinct(), then I highly envy you. In any case, you might find yourself using the duplicated() function when you need some more control. The function has a handy argument incomparables that allows you to ignore certain values when doing the comparison for duplicates.</description>
    </item>
    
    <item>
      <title>Cleaning Contact Info</title>
      <link>/2016/11/cleaning-contact-info/</link>
      <pubDate>Wed, 02 Nov 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/11/cleaning-contact-info/</guid>
      <description>Where are the Examples? Cleaning Functions Framework The Functions Cleaning Phone Numbers Cleaning URLs Cleaning Zip Codes Cleaning Email Addresses    Where are the Examples? R is great for cleaning data and that’s why I was surprised that I couldn’t find very many examples or even a full-fledged package for cleaning contact information (phone numbers, addresses, etc.). Instead I decided to write a few functions of my own to get the job done.</description>
    </item>
    
  </channel>
</rss>